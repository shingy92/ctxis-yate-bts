{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Practical2-GAN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shingy92/ctxis-yate-bts/blob/master/Practical2_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLHNA01ChQM3"
      },
      "source": [
        "<center><h1><b>Practical #2</b></h1>\n",
        "<h2><b>GANs: Generative Adversarial Networks</b><br/>\n",
        "<b>for Image Generation</b></h2></center>\n",
        "\n",
        "GANs use 2 neural networks trained on opposing loss functions: a **generator** which aims to produce data that is as similar as possible to the training data, and a **discriminator** which aims to distinguish between the real and the generated data. Please see lecture 4 for more details on how they work.\n",
        "\n",
        "<center><img width=600 src=\"https://drive.google.com/uc?id=1Z9PwIwUNjo9X1KxVHxCYe3YLcQboG6hq\"></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "During today's practical you will:\n",
        "1. Use given code to build the structure of the 2 networks, put together the GAN model and training loop and load datasets. \n",
        "2. Train the GAN and observe output results.\n",
        "3. Follow-up with exercises exploring the possibilities for customising the GAN, as well as the usage of pre-trained models and ready-made interactive applications using GANs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axlwf5zNwOcq"
      },
      "source": [
        "# Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4oq4kNb0m03"
      },
      "source": [
        "Create your own Jupyter notebook in Google Colab (or download a copy of this one so that you can edit it). Make sure that you enable GPU for the session (`Edit -> Notebook Settings -> Hardware accelerator -> GPU`). If you've made your own one, copy the code structure given here. Otherwise, just fill in the missing code.\n",
        "\n",
        "Next, import `tensorflow` into your project and check that everything is set up appropriately. The expected output from the following block of code is the name of the GPU (if nothing is printed after \"Found GPU at:\", then you don't have GPU access, please notify a demonstrator).\n",
        "\n",
        "We'll also perform the other required imports in this step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve7B5Sd52GVa"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "from IPython import display\n",
        "# import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEkSB2qpx_ha"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr5n6iS60tRl"
      },
      "source": [
        "The first thing we'll do in today's session is to retrieve a data set of images. The GAN that we will train next on this data set will output similar images. You can experiment with different options, but for now choose one of the following and store it in a variable:\n",
        "\n",
        "\n",
        "*   **MNIST**: 60000+10000 images of hand-drawn digits, split in 10 categories (0, 1, 2 etc.). Size 28x28, grayscale values 0-255. `tf.keras.datasets.mnist` ([More info](http://yann.lecun.com/exdb/mnist/))\n",
        "*   **Fashion MNIST**: 60000+10000 images of clothing items, split in 10 categories (top, trouser, pullover etc.). Size 28x28, RGB values 0-255. `tf.keras.datasets.fashion_mnist` ([More info](https://github.com/zalandoresearch/fashion-mnist))\n",
        "*   **Cifar10**: 50000+10000 images of objects, split in 10 categories (plane, bird, cat, dog etc.). Size 32x32, RGB values 0-255. `tf.keras.datasets.cifar10` ([More info](https://www.cs.toronto.edu/~kriz/cifar.html))\n",
        "\n",
        "The `load_data()` function applied to any of these datasets returns 4 NumPy arrays in 2 2-tuples:\n",
        "* `train_images` and `train_labels`: arrays which contain the training dataset, used by the GAN to learn. All images are labelled based on their category.\n",
        "* `test_images` and `test_labels`: arrays which contain the test set, to check the trained model's accuracy (and other statistics). We won't use this, we'll simply group train and test datasets together.\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "## <h1><img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-1:</b> Load a dataset</h1> \n",
        "\n",
        "<b>Choose</b> a dataset from the drop-down menu, which is then loaded into the 4 variables mentioned above: (train_images, train_labels), (test_images, test_labels). The images and labels are then grouped together into a single dataset.\n",
        "\n",
        "<i>Optional</i>: check the code to understand how the dataset is loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLuwT55303Id"
      },
      "source": [
        "dataset_choice = \"cifar10\"  # @param [\"mnist\", \"fashion_mnist\", \"cifar10\"]\n",
        "\n",
        "switcher = {\n",
        "    \"mnist\": tf.keras.datasets.mnist,\n",
        "    \"fashion_mnist\": tf.keras.datasets.fashion_mnist,\n",
        "    \"cifar10\": tf.keras.datasets.cifar10\n",
        "}\n",
        "dataset = switcher[dataset_choice]\n",
        "(train_images, train_labels), (test_images, test_labels) = dataset.load_data()\n",
        "\n",
        "train_images = np.concatenate((train_images, test_images))\n",
        "train_labels = np.concatenate((train_labels, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip6wLhpjKgpV"
      },
      "source": [
        "All datasets discussed here include several categories of objects. It will be easier for our GAN to learn to generate good images if we narrow down the *type* of images we're asking it to learn about. This is not necessary, but it could improve performance in the short tests we'll run here.\n",
        "\n",
        "## <h1><img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-2:</b> Select data category</h1> \n",
        "\n",
        "Please **run** the following block of code, use the drop-down that pops up to select the category you wish to use (from the previously selected dataset) and then run the following cell as well. These set up variables for use later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbrRh_h8IbW2"
      },
      "source": [
        "#@title Category chooser\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "categories = {\n",
        "    \"mnist\": [\"all\", 0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"fashion_mnist\": [\"all\", \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"],\n",
        "    \"cifar10\": [\"all\", \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "}\n",
        "\n",
        "category_picker = widgets.Dropdown(options=categories[dataset_choice], value='all')\n",
        "\n",
        "category_picker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE5dupj7_2g7"
      },
      "source": [
        "#@title Process chosen category\n",
        "\n",
        "category = categories[dataset_choice].index(category_picker.value)-1\n",
        "\n",
        "if category >= 0:\n",
        "  filter_train = np.where(train_labels == category)\n",
        "  filter_train = filter_train[0]\n",
        "  train_images = train_images[filter_train, :,:]\n",
        "\n",
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOvFm3Odh3_f"
      },
      "source": [
        "## Display training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO9bh8ViWmrk"
      },
      "source": [
        "Display some of the images to see what you've got."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4wvuiSbhkSF"
      },
      "source": [
        "#@title Helper function for displaying images\n",
        "\n",
        "def show_images(images):\n",
        "    if len(images.shape) > 3 and images.shape[3] == 1:\n",
        "        if tf.is_tensor(images):\n",
        "            images = images.numpy()\n",
        "        images = np.squeeze(images, axis=3)\n",
        "\n",
        "    num_images = len(images)\n",
        "    fig, cells = plt.subplots(ncols=num_images, nrows=1, figsize=(2 * num_images, 2))\n",
        "    for cell_num in range(num_images):\n",
        "        cells[cell_num].matshow(images[cell_num])\n",
        "        cells[cell_num].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEJngsesWp_1"
      },
      "source": [
        "#@title Display\n",
        "\n",
        "# Show only some of the images\n",
        "n_images =  32#@param{type: \"integer\"}\n",
        "images_display = train_images\n",
        "if n_images > 0:\n",
        "    images_display = images_display[:n_images]\n",
        "\n",
        "# Display!\n",
        "show_images(images_display)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgLUR8Q9FqRH"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "Let's keep track of some parameters related to our dataset. We recommend to keep `BATCH_SIZE` at 512, but you can play around with this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED8r7h_OBJ1G"
      },
      "source": [
        "#@title Dataset parameters\n",
        "\n",
        "# default: 512\n",
        "BATCH_SIZE = 512 # @param {type:\"slider\", min:128, max:1024, step:128}\n",
        "N_CHANNELS = (train_images.shape[-1] if len(train_images.shape)>3 else 1)\n",
        "TRAIN_BUF = len(train_images)\n",
        "IMG_SIZE = len(train_images[0])\n",
        "DIMS = (IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
        "N_TRAIN_BATCHES = int(TRAIN_BUF/BATCH_SIZE)\n",
        "\n",
        "print(\"Batch size: {} \\nTraining set size: {}\\\n",
        "       \\nImage Dimensions: {} \\n# Train batches: {}\".format(\n",
        "            BATCH_SIZE, TRAIN_BUF, DIMS, N_TRAIN_BATCHES\n",
        "        )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rqmn6NR64nG"
      },
      "source": [
        "### <h1><img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-3:</b> Normalize data</h1> \n",
        "\n",
        "Next we need to normalise the data between 0 and 1. To do this, first apply the `.astype(\"float32\")` function to the training dataset. Next, given that the colour layers go between 0 and 255, divide by 255.0 the train_images variable. **Implement** the code needed to achieve this.\n",
        "\n",
        "**Note**: Data should be reshaped appropriately, using e.g. `.reshape(train_images.shape[0], IMG_SIZE, IMG_SIZE, N_CHANNELS)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVt0Rx8uKXWF"
      },
      "source": [
        "#@title Normalize data\n",
        "\n",
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mgokRadGEnN"
      },
      "source": [
        "Next, we create batches for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuSRAsmmBLjV"
      },
      "source": [
        "#@title Batch datasets\n",
        "train_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(train_images)\n",
        "    .shuffle(TRAIN_BUF)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J37oVOWXA7m0"
      },
      "source": [
        "And that's it! our data is all set up and ready to be fed into the GAN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to1H12deGZ9r"
      },
      "source": [
        "# Generator Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcT3fzyMGex8"
      },
      "source": [
        "Let's start setting up our GAN, beginning with the structure of the generator network. The input for the generator is a vector of noise; from this, it will generate an image. The images will be random noise in the beginning, but the results will improve over time as the network is trained.\n",
        "\n",
        "We'll need the following layers for the generator network structure and an [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam):\n",
        "* **[Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)**: A densely connected layer, the first layer of our network.\n",
        "* **[Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)**: A convolutional layer that also upscales the image.\n",
        "* **[Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape)**: Used to change the shape of a vector.\n",
        "\n",
        "When you run the following code, it will output a summary of the network structure. The last layer output will have the same shape as the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRUbg3-_c2Hc"
      },
      "source": [
        "#@title Set up generator network structure\n",
        "\n",
        "\n",
        "# default: \"relu\"\n",
        "activation_l0 = \"relu\" #@param{type: \"string\"}\n",
        "# default: \"relu\"\n",
        "activation_l2 = \"relu\" #@param{type: \"string\"}\n",
        "# default: \"relu\"\n",
        "activation_l3 = \"relu\" #@param{type: \"string\"}\n",
        "# default: \"sigmoid\"\n",
        "activation_l4 = \"sigmoid\" #@param{type: \"string\"}\n",
        "\n",
        "n_stride = 2  # factor to scale image up by in each convolution layer\n",
        "n_layers_stride = 2  # number of layers scaling the image up\n",
        "n_smallest_dim = IMG_SIZE//(n_stride*n_layers_stride)\n",
        "seed_size = IMG_SIZE * (2 ** (n_layers_stride-1)) * N_CHANNELS\n",
        "\n",
        "# Generator structure, 5 layers: Dense, Reshape, 3xConvolutionTranspose\n",
        "generator = [\n",
        "    tf.keras.layers.Dense(units=n_smallest_dim * n_smallest_dim * seed_size, activation=activation_l0),\n",
        "    tf.keras.layers.Reshape(target_shape=(n_smallest_dim, n_smallest_dim, seed_size)),\n",
        "    tf.keras.layers.Conv2DTranspose(\n",
        "        filters=seed_size, kernel_size=3, strides=(n_stride, n_stride), padding=\"SAME\", activation=activation_l2\n",
        "    ),\n",
        "    tf.keras.layers.Conv2DTranspose(\n",
        "        filters=seed_size/2, kernel_size=3, strides=(n_stride, n_stride), padding=\"SAME\", activation=activation_l3\n",
        "    ),\n",
        "    tf.keras.layers.Conv2DTranspose(\n",
        "        filters=N_CHANNELS, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation=activation_l4\n",
        "    ),\n",
        "]\n",
        "\n",
        "#-- Optimiser\n",
        "# default: 0.001\n",
        "opt_learning_rate = 0.0001  #@param{type:\"raw\"}\n",
        "# default: 0.5\n",
        "opt_beta = 0.4 #@param{type:\"raw\"}\n",
        "gen_optimizer = tf.keras.optimizers.Adam(opt_learning_rate, beta_1=opt_beta)\n",
        "\n",
        "# Build the network\n",
        "gen = tf.keras.Sequential(generator)\n",
        "gen.build([train_images.shape[0], 1, 1, seed_size])  # specifies input shape\n",
        "\n",
        "# Print network summary\n",
        "gen.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgGntCsCGlMd"
      },
      "source": [
        "# Discriminator Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woB0GgHUGoKW"
      },
      "source": [
        "Next, let's define the structure of the discriminator network. What this network does is it receives an image, and it needs to decide whether the image is real or fake. So the output of this network is binary, real/fake. Some more concepts:\n",
        "\n",
        "*   **[InputLayer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer)**: A layer of a specific size, with input shape specified.\n",
        "*   **[Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)**: A convolutional layer.\n",
        "*   **[Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)**: Used to reshape a multi-dimensional vector into a 1D vector.\n",
        "*   **[RMSProp optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)**\n",
        "\n",
        "When you run the following code, it will also output a summary of the structure of the discriminator network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6RYXXrBdrd5"
      },
      "source": [
        "#@title Set up discriminator network structure\n",
        "\n",
        "# default: \"relu\"\n",
        "activation_l1 = \"relu\" #@param{type: \"string\"}\n",
        "# default: \"relu\"\n",
        "activation_l2 = \"relu\" #@param{type: \"string\"}\n",
        "\n",
        "# Discriminator structure: 5 layers, Input + Convolutionx2 + Flatten + Dense\n",
        "discriminator = [\n",
        "    tf.keras.layers.InputLayer(input_shape=DIMS),\n",
        "    tf.keras.layers.Conv2D(\n",
        "        filters=seed_size/2, kernel_size=3, strides=(n_stride, n_stride), activation=activation_l1\n",
        "    ),\n",
        "    tf.keras.layers.Conv2D(\n",
        "        filters=seed_size, kernel_size=3, strides=(n_stride, n_stride), activation=activation_l2\n",
        "    ),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=1, activation=None),\n",
        "]\n",
        "\n",
        "\n",
        "#-- Optimiser\n",
        "# default: 0.005\n",
        "opt_learning_rate = 0.0001  #@param{type:\"raw\"}\n",
        "disc_optimizer = tf.keras.optimizers.RMSprop(opt_learning_rate)\n",
        "\n",
        "# Build the network\n",
        "disc = tf.keras.Sequential(discriminator)\n",
        "disc.compile(loss='binary_crossentropy', optimizer= disc_optimizer , metrics = ['accuracy'])\n",
        "disc.build(DIMS)  # specifies input shape\n",
        "\n",
        "# Print network summary\n",
        "disc.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkobXar_Grh8"
      },
      "source": [
        "# Putting it all together: GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvuAnIbbtzfp"
      },
      "source": [
        "In this example, we set up the custom training loop for the GAN within a keras Model subclass. This will group together our 2 networks, and, during training, it will have the generator network generate images, which are then run through the discriminator together with an equal sample of real images. The results of this process are used to update both networks so that the generator learns to fool the discriminator into believing the fake images generated are real, while the discriminator learns to better distinguish real and fake images.\n",
        "\n",
        "Simply run the next block of code to put this together, and check that you understand the logic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWl9hrtnd6bK",
        "cellView": "code"
      },
      "source": [
        "#@title Set up the GAN as a Keras model object.\n",
        "\n",
        "class GAN(tf.keras.Model):\n",
        "    \"\"\" \n",
        "    A basic GAN class. Extends tf.keras.Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GAN, self).__init__()\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "        self.gen = self.gen\n",
        "        self.disc = self.disc\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "    def generate(self, z):\n",
        "        \"\"\"\n",
        "        Run input vector z through the generator to create fake data.\n",
        "        \"\"\"\n",
        "        return self.gen(z)\n",
        "\n",
        "    def discriminate(self, x):\n",
        "        \"\"\"\n",
        "        Run data through the discriminator to label it as real or fake.\n",
        "        \"\"\"\n",
        "        return self.disc(x)\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        \"\"\" \n",
        "        Passes through the network and computes loss for given data.\n",
        "        \"\"\"\n",
        "        # Generate a random vector seed from a uniform distribution\n",
        "        seed = tf.random.normal([x.shape[0], 1, 1, self.seed_size])\n",
        "\n",
        "        # Use the seed to generate a fake data set with the generator network.\n",
        "        fakes = self.generate(seed)\n",
        "\n",
        "        # Use the discriminator network to obtain labels for both the generated data (x_gen) and the real data (x)\n",
        "        logits_reals = self.discriminate(x)\n",
        "        logits_fakes = self.discriminate(fakes)\n",
        "\n",
        "        # Discriminator loss, looking at correctly labeled data\n",
        "        # Losses of the real data with correct label \"1\"\n",
        "        disc_real_loss = gan_loss(logits=logits_reals, is_real=True)\n",
        "        # Losses of the fake data with correct label \"0\"\n",
        "        disc_fake_loss = gan_loss(logits=logits_fakes, is_real=False)\n",
        "        # The discriminator loss is the sum of the 2 previous values\n",
        "        disc_loss = disc_fake_loss + disc_real_loss\n",
        "\n",
        "        # Generator loss, looking at the fake data labeled as real (\"1\")\n",
        "        gen_loss = gan_loss(logits=logits_fakes, is_real=True)\n",
        "\n",
        "        # Return losses\n",
        "        return disc_loss, gen_loss\n",
        "\n",
        "    def compute_gradients(self, x):\n",
        "        \"\"\" \n",
        "        Passes through the network and computes gradients.\n",
        "        \"\"\"\n",
        "        ### Pass x through network and compute losses\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            disc_loss, gen_loss = self.compute_loss(x)\n",
        "\n",
        "        # Compute gradients\n",
        "        gen_gradients = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "        disc_gradients = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
        "\n",
        "        return gen_gradients, disc_gradients\n",
        "\n",
        "    def apply_gradients(self, gen_gradients, disc_gradients):\n",
        "        \"\"\"\n",
        "        Apply given gradients to both networks.\n",
        "        \"\"\"\n",
        "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.gen.trainable_variables))\n",
        "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.disc.trainable_variables))\n",
        "\n",
        "    @tf.function\n",
        "    def train(self, train_x):\n",
        "        \"\"\"\n",
        "        Train the GAN! \n",
        "        \"\"\"\n",
        "        gen_gradients, disc_gradients = self.compute_gradients(train_x)\n",
        "        self.apply_gradients(gen_gradients, disc_gradients)\n",
        "        \n",
        "        \n",
        "def gan_loss(logits, is_real=True):\n",
        "    \"\"\"\n",
        "    Computes cross entropy between logits and labels\n",
        "    \"\"\"\n",
        "    if is_real:\n",
        "        labels = tf.ones_like(logits)\n",
        "    else:\n",
        "        labels = tf.zeros_like(logits)\n",
        "\n",
        "    # Returns loss calculation\n",
        "    return tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)\n",
        "\n",
        "\n",
        "# Set up the model\n",
        "\n",
        "model = GAN(\n",
        "    gen = gen,\n",
        "    disc = disc,\n",
        "    gen_optimizer = gen_optimizer,\n",
        "    disc_optimizer = disc_optimizer,\n",
        "    seed_size = seed_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hS1EUTtqJ3B"
      },
      "source": [
        "# GAN Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjWQCZWlucqx"
      },
      "source": [
        "While the GAN model set up takes care of a training iteration, we need to repeat this for several epochs and observe how the quality of images generated improves over time. The training is done in batches at each epoch, using the `tqdm` library to create these according to variables defined earlier and display progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g0RQqbBex0q"
      },
      "source": [
        "#@title Set up the training loop and run.\n",
        "\n",
        "# default: 50\n",
        "n_epochs =  100# @param{type: \"integer\"} \n",
        "\n",
        "# losses = pd.DataFrame(columns = ['disc_loss', 'gen_loss'])\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Train the model\n",
        "    for batch, train_x in tqdm(zip(range(N_TRAIN_BATCHES), train_dataset), total=N_TRAIN_BATCHES):\n",
        "        model.train(train_x)\n",
        "\n",
        "    # At this point we could also test the model, compute losses and display them during training (if we'd kept training and test images separate)\n",
        "    # loss = []\n",
        "    # for batch, test_x in tqdm(zip(range(N_TEST_BATCHES), test_dataset), total=N_TEST_BATCHES):\n",
        "    #     loss.append(model.compute_loss(train_x))\n",
        "    # losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
        "\n",
        "    # Display epoch and images generate at this point in training\n",
        "    display.clear_output()  # This line clears output between epochs\n",
        "    # print(\n",
        "    #     \"Epoch: {} | disc_loss: {} | gen_loss: {}\".format(\n",
        "    #         epoch, losses.disc_loss.values[-1], losses.gen_loss.values[-1]\n",
        "    #     )\n",
        "    # )\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    generated_images = model.gen(tf.random.normal(shape=(10, seed_size)))\n",
        "    show_images(generated_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udNZyi_Yu1kK"
      },
      "source": [
        "Once the model is trained (or even before!), we can run it! This can be done directly by using the generator network to generate new images, or by using the `predict()` function on the GAN model, which implicitly calls the generator network. The input here is a random noise vector, and the output is a set of generated images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg3_i32GUMJ9"
      },
      "source": [
        "#@title Run the (trained) model\n",
        "\n",
        "# Running with random input to generate images\n",
        "samples = model.predict(tf.random.normal(shape=(10, 1, 1, seed_size)))\n",
        "show_images(samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMoQnQT5mJN6"
      },
      "source": [
        "# Saving and Loading models\n",
        "\n",
        "In this example we're subclassing the keras Model class and adding several variables and methods to it to facilitate the definition of the custom training loop typical in GANs. Because of this, tensorflow does not provide an easy way to save the whole GAN model after it's been trained.\n",
        "\n",
        "However, we do have defined 2 networks within our GAN, while the GAN model simply puts them together and takes care of the appropriate training loop. As a result, what we *can* do is **save** the 2 networks individually. Then, when we **load** them, either use the generator network directly to generate images, or put the 2 networks back together in the GAN class for more training or testing as needed.\n",
        "\n",
        "We're going to be saving these models in Google Drive. So, first thing to do is mount the drive and choose file paths within it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eONE9GSoK4zV"
      },
      "source": [
        "#@title Save model to GDrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "gen_save_name = 'gen' #@param{type: 'string'}\n",
        "gen_save_name += '.tf'  # Set extension separately\n",
        "disc_save_name = 'disc' #@param{type: 'string'}\n",
        "disc_save_name += '.tf' # Set extension separately\n",
        "\n",
        "path = 'My Drive/Work/Colab/' #@param{type: 'string'}\n",
        "full_path_g = F\"/content/gdrive/{path}{gen_save_name}\" \n",
        "full_path_d = F\"/content/gdrive/{path}{disc_save_name}\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYIJFkcpnXdq"
      },
      "source": [
        "Next, we save the 2 networks using the `save(path, save_format='tf')` function on the model objects themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA7iFwGvnxWy"
      },
      "source": [
        "#@title Save model\n",
        "\n",
        "# tf.saved_model.save(gen, full_path_g)\n",
        "gen.save(full_path_g, save_format='tf')\n",
        "disc.save(full_path_d, save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNMJasvQn1QJ"
      },
      "source": [
        "We can then load these models (potentially in a different notebook altogether!) using the `tf.keras.models.load_model(path)` function.\n",
        "\n",
        "In this example we're putting the 2 networks together into the GAN model and use it to generate some images. The quality of the images obtained should be the same as after training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l9OrGkOk5Ny"
      },
      "source": [
        "#@title Load model and generate images\n",
        "gen2 = tf.keras.models.load_model(full_path_g)\n",
        "disc2 = tf.keras.models.load_model(full_path_d)\n",
        "\n",
        "# Put together GAN model with the loaded networks\n",
        "model = GAN(\n",
        "    gen = gen2,\n",
        "    disc = disc2,\n",
        "    gen_optimizer = gen_optimizer,\n",
        "    disc_optimizer = disc_optimizer,\n",
        "    seed_size = seed_size\n",
        ")\n",
        "\n",
        "# Generate some images!\n",
        "samples = model.predict(tf.random.normal(shape=(10, 1, 1, seed_size)))\n",
        "show_images(samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1mDE680qyxu"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw3idx4uq619"
      },
      "source": [
        "## <img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-4:</b> Customize the network\n",
        "\n",
        "1. You will find several **parameters** in this notebook, highlighted on the right side of the code blocks (e.g. learning rate for the optimisers). If you modify some of these values, how do you think it will impact the images generated by the GAN? \n",
        "  <ol type=\"a\">\n",
        "  <li>Are they better, or worse? </li>\n",
        "  <li>Does it need more or less training time to start generating good images?  </li>\n",
        "  <li>Try some different values and check if your intuition is correct.</li>\n",
        "  </ol>\n",
        "\n",
        "2. The **structure** of the networks can also be modified.  \n",
        "\n",
        "  <ol type=\"a\">\n",
        "  <li>Look into the different activation functions available and check if others work better or worse in this context.</li>\n",
        "  <li>What other optimisers could be used for better performance?</li>\n",
        "  <li>What about the layers? Would adding <i>batch normalization</i> or <i>dropout</i> make a difference? How can you add these to the network structures? (<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers\">Hint</a>)  </li>\n",
        "  </ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TniLKvZYbIhj"
      },
      "source": [
        "## <img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-5:</b> Experiment with pre-trained models\n",
        "\n",
        "1. Copy these trained [generator model](https://drive.google.com/drive/folders/1MKuKWB1QXNcKbenmPRN0QWSkhbJAw4iH?usp=sharing) and [discriminator model](https://drive.google.com/drive/folders/1oWNuQKf_PdjFDxZmuuzFfdlhyMVPm1cB?usp=sharing) to your own GDrive. The models were trained on the fashion-MNIST (tops) dataset with the same default settings found in this notebook, for 500 epochs. What kind of images does the resultant GAN generate? \n",
        "\n",
        "2. If you gave it a different dataset (e.g. fashion-MNIST sandals) and trained it for 50 more epochs, does it output images that look like tops, sandals, or a mash-up of the 2?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU16ZsAIuwOg"
      },
      "source": [
        "## <img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-6:</b> Load your own data\n",
        "\n",
        "1. Use the code below to load your own images from your Google Drive into `train_images` and `test_images` variables (the code uses a 80% - 20% split between training and test images). Alternatively, you can write your own code to load images from your own machine.\n",
        "2. Run the rest of the code in the notebook again (from the data pre-processing step) to train the GAN with your own images instead and observe output generated. You may need to adjust some parameters, such as the batch size, so that everything works correctly with your data.\n",
        "\n",
        "**Note**: make sure all of your images have the same size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9pA6GJcu2a6"
      },
      "source": [
        "# @title Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGEzgra1yIPd"
      },
      "source": [
        "#@title Unzip file (if using zip file)\n",
        "\n",
        "# The path to a zip of images\n",
        "path = 'My Drive/Work/Colab/' #@param{type: 'string'}\n",
        "\n",
        "# File name\n",
        "zip_file_name = \"images\" #@param{type: 'string'}\n",
        "full_path = '/content/gdrive/' + path + filename + \".zip\"\n",
        "full_path_dir = '/content/gdrive/' + path + filename + \"/\" \n",
        "   \n",
        "!unzip full_path -d full_path_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8rpepnFywdu"
      },
      "source": [
        "#@title Setting paths to folder (if NOT using zip file)\n",
        "\n",
        "path = 'My Drive/Work/Colab/' #@param{type: 'string'}\n",
        "folder_name = \"images\" #@param{type: 'string'}\n",
        "full_path_dir = '/content/gdrive/' + path + folder_name + \"/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1izRRLK-yWS5"
      },
      "source": [
        "#@title Open directory containing images and load data\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "data = []\n",
        "\n",
        "for filename in os.listdir(full_path_dir):\n",
        "    image = Image.open(full_path_dir + filename)\n",
        "    data.append(np.asarray(image))\n",
        "\n",
        "train_images = np.array(data)\n",
        "print(train_images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biu-4xvOcHpc"
      },
      "source": [
        "## <img width=30 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Bright_green_checkbox-checked.svg/1024px-Bright_green_checkbox-checked.svg.png\"> <b>TODO-7:</b> Test online GAN applications\n",
        "\n",
        "1. [GAN Lab](https://poloclub.github.io/ganlab/) offers a browser-based visualisation of a GAN training over time. Play around with the settings (including editting the network structure by clicking the pen icon next to the \"Model Overview Graph\" in the top-left corner) and observe the quality of the output.\n",
        "2. [Pix2Pix-TF](https://affinelayer.com/pixsrv/) is a tensorflow implementation of Pix2Pix. The online demo shows how simple sketches can be transformed into cat pictures, buildings, shoes or bags.\n",
        "3. [GANpaint](http://gandissect.res.ibm.com/ganpaint.html?project=churchoutdoor&layer=layer4) shows how GANs can be used to add elements into an existing image.\n",
        "2. [Pix2Vox](https://github.com/maxorange/pix2vox) is an open-source application which uses GANs to generate 3D models from simple sketches. You can download it and test what type of models you can make it generate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZm3WrETph3A"
      },
      "source": [
        "# Further reading\n",
        "\n",
        "- Ways to improve GAN performance: https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIrtovFTVa2l"
      },
      "source": [
        "## Related tutorials\n",
        "\n",
        "Some of the materials in this practical were based on the following tutorials:\n",
        "\n",
        "* Ander Fernandez Jauregui, [How to code GAN in Python](https://anderfernandez.com/en/blog/how-to-code-gan-in-python/)\n",
        "* Tim Sainburg, [GAN Fashion-MNIST](https://colab.research.google.com/github/timsainb/tensorflow2-generative-models/blob/master/2.0-GAN-fashion-mnist.ipynb)\n",
        "* [TF-GAN tutorial](https://colab.research.google.com/github/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb)"
      ]
    }
  ]
}